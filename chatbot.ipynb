{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_copy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "B6TodplOvqRG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Building a ChatBot with Deep NLP\n",
        " \n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from google.colab import files\n",
        " \n",
        " \n",
        "########## PART 1 - DATA PREPROCESSING ##########\n",
        " \n",
        " \n",
        " \n",
        "# Importing the dataset\n",
        "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        " \n",
        "# Creating a dictionary that maps each line and its id\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        " \n",
        "# Creating a list of all of the conversations\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))\n",
        " \n",
        "# Getting separately the questions and the answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "    for i in range(len(conversation) - 1):\n",
        "        questions.append(id2line[conversation[i]])\n",
        "        answers.append(id2line[conversation[i+1]])\n",
        " \n",
        "# Doing a first cleaning of the texts\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text\n",
        " \n",
        "# Cleaning the questions\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        " \n",
        "# Cleaning the answers\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))\n",
        " \n",
        "# Filtering out the questions and answers that are too short or too long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "i = 0\n",
        "for question in clean_questions:\n",
        "    if 2 <= len(question.split()) <= 25:\n",
        "        short_questions.append(question)\n",
        "        short_answers.append(clean_answers[i])\n",
        "    i += 1\n",
        "clean_questions = []\n",
        "clean_answers = []\n",
        "i = 0\n",
        "for answer in short_answers:\n",
        "    if 2 <= len(answer.split()) <= 25:\n",
        "        clean_answers.append(answer)\n",
        "        clean_questions.append(short_questions[i])\n",
        "    i += 1\n",
        " \n",
        "# Creating a dictionary that maps each word to its number of occurrences\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        " \n",
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\n",
        "threshold_questions = 15\n",
        "questionswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_questions:\n",
        "        questionswords2int[word] = word_number\n",
        "        word_number += 1\n",
        "threshold_answers = 15\n",
        "answerswords2int = {}\n",
        "word_number = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold_answers:\n",
        "        answerswords2int[word] = word_number\n",
        "        word_number += 1\n",
        " \n",
        "# Adding the last tokens to these two dictionaries\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "for token in tokens:\n",
        "    questionswords2int[token] = len(questionswords2int) + 1\n",
        "for token in tokens:\n",
        "    answerswords2int[token] = len(answerswords2int) + 1\n",
        " \n",
        "# Creating the inverse dictionary of the answerswords2int dictionary\n",
        "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
        " \n",
        "# Adding the End Of String token to the end of every answer\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += ' <EOS>'\n",
        " \n",
        "# Translating all the questions and the answers into integers\n",
        "# and Replacing all the words that were filtered out by <OUT> \n",
        "questions_into_int = []\n",
        "for question in clean_questions:\n",
        "    ints = []\n",
        "    for word in question.split():\n",
        "        if word not in questionswords2int:\n",
        "            ints.append(questionswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(questionswords2int[word])\n",
        "    questions_into_int.append(ints)\n",
        "answers_into_int = []\n",
        "for answer in clean_answers:\n",
        "    ints = []\n",
        "    for word in answer.split():\n",
        "        if word not in answerswords2int:\n",
        "            ints.append(answerswords2int['<OUT>'])\n",
        "        else:\n",
        "            ints.append(answerswords2int[word])\n",
        "    answers_into_int.append(ints)\n",
        " \n",
        "# Sorting questions and answers by the length of questions\n",
        "sorted_clean_questions = []\n",
        "sorted_clean_answers = []\n",
        "for length in range(1, 25 + 1):\n",
        "    for i in enumerate(questions_into_int):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
        "            sorted_clean_answers.append(answers_into_int[i[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xPw-wq3r2hKM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## PART 2 - BUILDING THE SEQ2SEQ MODEL ##########\n",
        " \n",
        " \n",
        " \n",
        "# Creating placeholders for the inputs and the targets\n",
        "def model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
        "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
        "    target_seq_len=tf.placeholder(tf.int32,[None],name=\"target_sequence_length\")\n",
        "    max_target_seq_len=tf.reduce_max(target_seq_len)\n",
        "    source_seq_len=tf.placeholder(tf.int32,[None],name=\"source_sequence_length\")\n",
        "    return inputs,targets,lr,keep_prob,target_seq_len,max_target_seq_len,source_seq_len\n",
        " \n",
        "# Preprocessing the targets\n",
        "def preprocess_targets(targets, word2int, batch_size):\n",
        "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
        "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1])\n",
        "    preprocessed_targets = tf.concat([left_side, right_side], 1)\n",
        "    return preprocessed_targets\n",
        " \n",
        "# Creating the Encoder RNN\n",
        "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
        "                   source_sequence_length, source_vocab_size, \n",
        "                   encoding_embedding_size):\n",
        "    embed_input = tf.contrib.layers.embed_sequence(rnn_inputs,\n",
        "                                                   vocab_size=source_vocab_size,\n",
        "                                                   embed_dim=encoding_embedding_size)\n",
        "    \n",
        "    def lstm_cell():\n",
        "        cell = tf.nn.rnn_cell.LSTMCell(num_units=rnn_size,name='basic_lstm_cell')\n",
        "        lstm_cell=tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
        "        return lstm_cell\n",
        "    \n",
        "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
        "                    [lstm_cell() for _ in range(num_layers)])\n",
        "    enc_output,enc_FinalState = tf.nn.dynamic_rnn(stacked_lstm,\n",
        "                                          embed_input,\n",
        "                                          source_sequence_length,\n",
        "                                          dtype=tf.float32)\n",
        "    return enc_output,enc_FinalState\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2yOBo1uFxce_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_train(encoder_output,encoder_state, dec_cell, dec_embed_input, \n",
        "                         source_sequence_length,target_sequence_length, max_summary_length, \n",
        "                         output_layer, rnn_size,keep_prob,beam_width):\n",
        "    \n",
        "    with tf.variable_scope(\"myScope\"):\n",
        "\n",
        "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units = rnn_size,\n",
        "                                                                memory = encoder_output,\n",
        "                                                                memory_sequence_length = source_sequence_length)\n",
        "        dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "        cell = dec_cell,\n",
        "        attention_mechanism = attention_mechanism,\n",
        "        attention_layer_size = rnn_size/2)\n",
        "\n",
        "        attn_zero = dec_cell.zero_state(batch_size,tf.float32)\n",
        "        attn_zero = attn_zero.clone(cell_state=encoder_state)\n",
        "\n",
        "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                            sequence_length=target_sequence_length,\n",
        "                                                            time_major=False)\n",
        "\n",
        "\n",
        "        # Basic decoder\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                           training_helper,\n",
        "                                                           attn_zero,\n",
        "                                                           output_layer) \n",
        "\n",
        "        # Perform dynamic decoding using the decoder\n",
        "        training_decoder_output= tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                                       impute_finished=True,\n",
        "                                                                       output_time_major=False,\n",
        "                                                                       maximum_iterations=max_summary_length)[0]\n",
        "\n",
        "        return training_decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PsH7WEMpxcot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer_infer(encoder_output,encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
        "                         end_of_sequence_id, source_sequence_length,max_target_sequence_length,\n",
        "                         vocab_size, output_layer, batch_size,rnn_size, keep_prob,beam_width):\n",
        "    with tf.variable_scope(\"myScope\",reuse=True):\n",
        "        tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n",
        "        encoder_output, multiplier=beam_width)\n",
        "        tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(\n",
        "            encoder_state, multiplier=beam_width)\n",
        "        tiled_sequence_length = tf.contrib.seq2seq.tile_batch(\n",
        "            source_sequence_length, multiplier=beam_width)\n",
        "        \n",
        "        attention_mechanism_beam = tf.contrib.seq2seq.LuongAttention(num_units = rnn_size,\n",
        "                                                                     memory = tiled_encoder_outputs,\n",
        "                                                                     memory_sequence_length = tiled_sequence_length)\n",
        "        dec_cell_beam = tf.contrib.seq2seq.AttentionWrapper(\n",
        "        cell = dec_cell,\n",
        "        attention_mechanism = attention_mechanism_beam,\n",
        "        attention_layer_size = rnn_size/2)\n",
        "\n",
        "        attn_zero_beam = dec_cell_beam.zero_state(batch_size * beam_width,tf.float32)\n",
        "        attn_zero_beam = attn_zero_beam.clone(cell_state=tiled_encoder_final_state)\n",
        "\n",
        "        start_of_sequence_ids = tf.tile([start_of_sequence_id], [batch_size])\n",
        "\n",
        "        # Basic decoder\n",
        "        inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder( dec_cell_beam,\n",
        "                                                           embedding = dec_embeddings,\n",
        "                                                           start_tokens = start_of_sequence_ids,\n",
        "                                                           end_token = end_of_sequence_id,\n",
        "                                                           initial_state = attn_zero_beam,\n",
        "                                                           beam_width = beam_width\n",
        "                                                           ,output_layer=output_layer)\n",
        "\n",
        "        # Perform dynamic decoding using the decoder\n",
        "        inference_decoder_output,t1,t2 = tf.contrib.seq2seq.dynamic_decode(inference_decoder,               \n",
        "                                                            maximum_iterations=max_target_sequence_length)\n",
        "\n",
        "        beam_logits = tf.no_op()\n",
        "        return inference_decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDAm3H3wxcxD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decoding_layer(dec_input,encoder_output, encoder_state,\n",
        "                   source_sequence_length,target_sequence_length,\n",
        "                   max_target_sequence_length,rnn_size,\n",
        "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, decoding_embedding_size,beam_width):\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    def lstm_cell():\n",
        "        cell = tf.nn.rnn_cell.LSTMCell(num_units=rnn_size,name='basic_lstm_cell')\n",
        "        \n",
        "        lstm_cell=tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
        "        return lstm_cell\n",
        "    \n",
        "    dec_cell = tf.contrib.rnn.MultiRNNCell(\n",
        "                    [lstm_cell() for _ in range(num_layers)])\n",
        "    output_layer = Dense(target_vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "\n",
        "    with tf.variable_scope(\"decode\") as scope:\n",
        "        training_decoder_output = decoding_layer_train(encoder_output,encoder_state, dec_cell, dec_embed_input,\n",
        "                                                       source_sequence_length,target_sequence_length, max_target_sequence_length, \n",
        "                                                       output_layer, rnn_size,keep_prob,beam_width)\n",
        "    \n",
        "        inference_decoder_output = decoding_layer_infer(encoder_output,encoder_state, dec_cell, dec_embeddings, target_vocab_to_int['<SOS>'], \n",
        "                                                        target_vocab_to_int['<EOS>'],source_sequence_length, max_target_sequence_length,\n",
        "                                                        target_vocab_size, output_layer, batch_size, rnn_size, keep_prob,beam_width)\n",
        "    \n",
        "    return training_decoder_output, inference_decoder_output\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "37XXuRv4xc60",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
        "                  source_sequence_length, target_sequence_length,\n",
        "                  max_target_sentence_length,\n",
        "                  source_vocab_size, target_vocab_size,\n",
        "                  enc_embedding_size, dec_embedding_size,\n",
        "                  rnn_size, num_layers, target_vocab_to_int,beam_width):\n",
        "    encoder_output, enc_state = encoding_layer(input_data, \n",
        "                                  rnn_size, \n",
        "                                  num_layers, \n",
        "                                  keep_prob,\n",
        "                                  source_sequence_length,\n",
        "                                  source_vocab_size, \n",
        "                                  enc_embedding_size)\n",
        "    \n",
        "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
        "    dec_input = preprocess_targets(target_data, target_vocab_to_int, batch_size)\n",
        "    \n",
        "    # Pass encoder state and decoder inputs to the decoders\n",
        "    training_decoder_output, inference_decoder_output = decoding_layer(dec_input,\n",
        "                                                                       encoder_output,\n",
        "                                                                       enc_state,\n",
        "                                                                       source_sequence_length,\n",
        "                                                                       target_sequence_length,\n",
        "                                                                       max_target_sentence_length,\n",
        "                                                                       rnn_size,\n",
        "                                                                       num_layers, \n",
        "                                                                       target_vocab_to_int,\n",
        "                                                                       target_vocab_size,\n",
        "                                                                       batch_size,\n",
        "                                                                       keep_prob,\n",
        "                                                                       dec_embedding_size,\n",
        "                                                                       beam_width) \n",
        "    \n",
        "    return training_decoder_output, inference_decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eE56F3kAyIwM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Setting the Hyperparameters\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "rnn_size = 1024\n",
        "num_layers = 3\n",
        "beam_width=5\n",
        "encoding_embedding_size = 1024\n",
        "decoding_embedding_size = 1024\n",
        "learning_rate = 0.001\n",
        "learning_rate_decay = 0.9\n",
        "min_learning_rate = 0.0001\n",
        "keep_probability = 0.5\n",
        "max_target_sentence_length = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wPRJnGBjyI4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defining a session\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        " \n",
        "      # Loading the model inputs\n",
        "      inputs,targets,lr,keep_prob,target_seq_len,max_target_seq_len,source_seq_len = model_inputs()\n",
        "\n",
        "      # Getting the shape of the inputs tensor\n",
        "      input_shape = tf.shape(inputs)\n",
        "\n",
        "      # Getting the training and test predictions\n",
        "      training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                         targets,\n",
        "                                                         keep_prob,\n",
        "                                                         batch_size,\n",
        "                                                         source_seq_len,\n",
        "                                                         target_seq_len,\n",
        "                                                         max_target_seq_len,\n",
        "                                                         len(answerswords2int),\n",
        "                                                         len(questionswords2int),\n",
        "                                                         encoding_embedding_size,\n",
        "                                                         decoding_embedding_size,\n",
        "                                                         rnn_size,\n",
        "                                                         num_layers,\n",
        "                                                         questionswords2int,\n",
        "                                                         beam_width)\n",
        "      training_logits = tf.identity(training_predictions.rnn_output, name='logits')\n",
        "      inference_logits = tf.identity(test_predictions.predicted_ids, name='predictions')\n",
        "\n",
        "      masks = tf.sequence_mask(target_seq_len, max_target_seq_len, dtype=tf.float32, name='masks')\n",
        "\n",
        "      \n",
        "      with tf.name_scope(\"optimization\"):\n",
        "          # Loss function\n",
        "          cost = tf.contrib.seq2seq.sequence_loss(\n",
        "              training_logits,\n",
        "              targets,\n",
        "              masks)\n",
        "\n",
        "          # Optimizer\n",
        "          optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "          # Gradient Clipping\n",
        "          gradients = optimizer.compute_gradients(cost)\n",
        "          capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "          train_op = optimizer.apply_gradients(capped_gradients)\n",
        "      saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gHr2v5ws35F4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(sources)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:start_i + batch_size]\n",
        "        targets_batch = targets[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
        "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in pad_targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in pad_sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "im37RIyzDO6_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split data to training and validation sets\n",
        "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
        "train_source = sorted_clean_questions[training_validation_split:]\n",
        "train_target = sorted_clean_answers[training_validation_split:]\n",
        "valid_source = sorted_clean_questions[:training_validation_split]\n",
        "valid_target =sorted_clean_answers[:training_validation_split]\n",
        "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
        "                                                                                                             valid_target,\n",
        "                                                                                                             batch_size,\n",
        "                                                                                                             questionswords2int['<PAD>'],\n",
        "                                                                                                             answerswords2int['<PAD>']))                                                                                                  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QUES_viUDKkz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training\n",
        "batch_index_check_training_loss =500\n",
        "batch_index_check_validation_loss = ((len(train_source)) // batch_size // 2) - 1\n",
        "total_training_loss_error = 0\n",
        "list_validation_loss_error = []\n",
        "early_stopping_check = 0\n",
        "early_stopping_stop = 100\n",
        "checkpoint = \"checkpoints/chatbot_weights.ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oY3b8hx_7Ib9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2737
        },
        "outputId": "8bbdfc36-8152-46f4-d630-a1e4d8e513d2"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
        "                get_batches(train_source, train_target, batch_size,\n",
        "                            questionswords2int['<PAD>'],\n",
        "                            answerswords2int['<PAD>'])):\n",
        "            starting_time = time.time()\n",
        "            _, batch_training_loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {inputs: source_batch,\n",
        "                 targets: target_batch,\n",
        "                 lr: learning_rate,\n",
        "                 target_seq_len: targets_lengths,\n",
        "                 source_seq_len: sources_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "            total_training_loss_error += batch_training_loss\n",
        "            ending_time = time.time()\n",
        "            batch_time = ending_time - starting_time\n",
        "\n",
        "            if batch_i % batch_index_check_training_loss == 0 and batch_i > 0:\n",
        "                 print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
        "                                                                                                                                       epochs,\n",
        "                                                                                                                                       batch_i,\n",
        "                                                                                                                                       len(train_source) // batch_size,\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
        "                 total_training_loss_error = 0\n",
        "\n",
        "\n",
        "            if batch_i % batch_index_check_validation_loss == 0 and batch_i > 0:\n",
        "                  total_validation_loss_error = 0\n",
        "                  for batch_v, (valid_batch, target_batch, valid_lengths, targets_lengths) in enumerate(get_batches(valid_source, valid_target, batch_size,questionswords2int['<PAD>'],answerswords2int['<PAD>'])):\n",
        "                        _, batch_validation_loss = sess.run(\n",
        "                            [train_op, cost],\n",
        "                            {inputs: valid_batch,\n",
        "                             targets: target_batch,\n",
        "                             lr: learning_rate,\n",
        "                             target_seq_len: targets_lengths,\n",
        "                             source_seq_len: valid_lengths,\n",
        "                             keep_prob: keep_probability})\n",
        "                        total_validation_loss_error += batch_validation_loss\n",
        "                  ending_time = time.time()\n",
        "                  batch_time = ending_time - starting_time\n",
        "\n",
        "                  average_validation_loss_error = total_validation_loss_error / (len(valid_source) / batch_size)\n",
        "                  print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
        "                  learning_rate *= learning_rate_decay\n",
        "                  if learning_rate < min_learning_rate:\n",
        "                      learning_rate = min_learning_rate\n",
        "                  list_validation_loss_error.append(average_validation_loss_error)\n",
        "                  if average_validation_loss_error <= min(list_validation_loss_error):\n",
        "                      print('I speak better now!!')\n",
        "                      early_stopping_check = 0\n",
        "                      saver.save(sess, checkpoint)\n",
        "                      #files.download(checkpoint+'.meta')\n",
        "                      print('Model Trained and Saved')\n",
        "                  else:\n",
        "                      print(\"Sorry I do not speak better, I need to practice more.\")\n",
        "                      early_stopping_check += 1\n",
        "                      if early_stopping_check == early_stopping_stop:\n",
        "                          break\n",
        "        if early_stopping_check == early_stopping_stop:\n",
        "            print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\n",
        "            break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   0/20, Batch:  500/1030, Training Loss Error:  2.171, Training Time on 100 Batches: 399 seconds\n",
            "Validation Loss Error:  1.871, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   0/20, Batch: 1000/1030, Training Loss Error:  1.880, Training Time on 100 Batches: 546 seconds\n",
            "Validation Loss Error:  1.755, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   1/20, Batch:  500/1030, Training Loss Error:  1.859, Training Time on 100 Batches: 398 seconds\n",
            "Validation Loss Error:  1.693, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   1/20, Batch: 1000/1030, Training Loss Error:  1.756, Training Time on 100 Batches: 542 seconds\n",
            "Validation Loss Error:  1.651, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   2/20, Batch:  500/1030, Training Loss Error:  1.777, Training Time on 100 Batches: 399 seconds\n",
            "Validation Loss Error:  1.612, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   2/20, Batch: 1000/1030, Training Loss Error:  1.698, Training Time on 100 Batches: 545 seconds\n",
            "Validation Loss Error:  1.580, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   3/20, Batch:  500/1030, Training Loss Error:  1.728, Training Time on 100 Batches: 401 seconds\n",
            "Validation Loss Error:  1.549, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   3/20, Batch: 1000/1030, Training Loss Error:  1.657, Training Time on 100 Batches: 539 seconds\n",
            "Validation Loss Error:  1.520, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   4/20, Batch:  500/1030, Training Loss Error:  1.692, Training Time on 100 Batches: 401 seconds\n",
            "Validation Loss Error:  1.490, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   4/20, Batch: 1000/1030, Training Loss Error:  1.625, Training Time on 100 Batches: 540 seconds\n",
            "Validation Loss Error:  1.462, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   5/20, Batch:  500/1030, Training Loss Error:  1.663, Training Time on 100 Batches: 397 seconds\n",
            "Validation Loss Error:  1.435, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   5/20, Batch: 1000/1030, Training Loss Error:  1.598, Training Time on 100 Batches: 543 seconds\n",
            "Validation Loss Error:  1.410, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   6/20, Batch:  500/1030, Training Loss Error:  1.637, Training Time on 100 Batches: 398 seconds\n",
            "Validation Loss Error:  1.386, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   6/20, Batch: 1000/1030, Training Loss Error:  1.575, Training Time on 100 Batches: 545 seconds\n",
            "Validation Loss Error:  1.362, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   7/20, Batch:  500/1030, Training Loss Error:  1.617, Training Time on 100 Batches: 400 seconds\n",
            "Validation Loss Error:  1.341, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   7/20, Batch: 1000/1030, Training Loss Error:  1.553, Training Time on 100 Batches: 541 seconds\n",
            "Validation Loss Error:  1.321, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   8/20, Batch:  500/1030, Training Loss Error:  1.599, Training Time on 100 Batches: 398 seconds\n",
            "Validation Loss Error:  1.305, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   8/20, Batch: 1000/1030, Training Loss Error:  1.534, Training Time on 100 Batches: 546 seconds\n",
            "Validation Loss Error:  1.286, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   9/20, Batch:  500/1030, Training Loss Error:  1.584, Training Time on 100 Batches: 399 seconds\n",
            "Validation Loss Error:  1.272, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:   9/20, Batch: 1000/1030, Training Loss Error:  1.518, Training Time on 100 Batches: 538 seconds\n",
            "Validation Loss Error:  1.257, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  10/20, Batch:  500/1030, Training Loss Error:  1.571, Training Time on 100 Batches: 400 seconds\n",
            "Validation Loss Error:  1.245, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  10/20, Batch: 1000/1030, Training Loss Error:  1.502, Training Time on 100 Batches: 538 seconds\n",
            "Validation Loss Error:  1.233, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  11/20, Batch:  500/1030, Training Loss Error:  1.561, Training Time on 100 Batches: 399 seconds\n",
            "Validation Loss Error:  1.224, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  11/20, Batch: 1000/1030, Training Loss Error:  1.490, Training Time on 100 Batches: 544 seconds\n",
            "Validation Loss Error:  1.215, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  12/20, Batch:  500/1030, Training Loss Error:  1.552, Training Time on 100 Batches: 398 seconds\n",
            "Validation Loss Error:  1.208, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  12/20, Batch: 1000/1030, Training Loss Error:  1.479, Training Time on 100 Batches: 541 seconds\n",
            "Validation Loss Error:  1.200, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  13/20, Batch:  500/1030, Training Loss Error:  1.544, Training Time on 100 Batches: 401 seconds\n",
            "Validation Loss Error:  1.192, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  13/20, Batch: 1000/1030, Training Loss Error:  1.469, Training Time on 100 Batches: 542 seconds\n",
            "Validation Loss Error:  1.184, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  14/20, Batch:  500/1030, Training Loss Error:  1.537, Training Time on 100 Batches: 398 seconds\n",
            "Validation Loss Error:  1.177, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  14/20, Batch: 1000/1030, Training Loss Error:  1.459, Training Time on 100 Batches: 542 seconds\n",
            "Validation Loss Error:  1.168, Batch Validation Time: 125 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  15/20, Batch:  500/1030, Training Loss Error:  1.530, Training Time on 100 Batches: 394 seconds\n",
            "Validation Loss Error:  1.163, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  15/20, Batch: 1000/1030, Training Loss Error:  1.448, Training Time on 100 Batches: 537 seconds\n",
            "Validation Loss Error:  1.154, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  16/20, Batch:  500/1030, Training Loss Error:  1.523, Training Time on 100 Batches: 402 seconds\n",
            "Validation Loss Error:  1.147, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  16/20, Batch: 1000/1030, Training Loss Error:  1.438, Training Time on 100 Batches: 534 seconds\n",
            "Validation Loss Error:  1.138, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  17/20, Batch:  500/1030, Training Loss Error:  1.516, Training Time on 100 Batches: 396 seconds\n",
            "Validation Loss Error:  1.131, Batch Validation Time: 123 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  17/20, Batch: 1000/1030, Training Loss Error:  1.427, Training Time on 100 Batches: 539 seconds\n",
            "Validation Loss Error:  1.124, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  18/20, Batch:  500/1030, Training Loss Error:  1.510, Training Time on 100 Batches: 396 seconds\n",
            "Validation Loss Error:  1.118, Batch Validation Time: 123 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  18/20, Batch: 1000/1030, Training Loss Error:  1.416, Training Time on 100 Batches: 536 seconds\n",
            "Validation Loss Error:  1.109, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  19/20, Batch:  500/1030, Training Loss Error:  1.504, Training Time on 100 Batches: 394 seconds\n",
            "Validation Loss Error:  1.102, Batch Validation Time: 124 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n",
            "Epoch:  19/20, Batch: 1000/1030, Training Loss Error:  1.406, Training Time on 100 Batches: 541 seconds\n",
            "Validation Loss Error:  1.096, Batch Validation Time: 123 seconds\n",
            "I speak better now!!\n",
            "Model Trained and Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XB9mVWMshbhy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e3f23cf6-e134-4806-98e3-d6f2b0ecf1a1"
      },
      "cell_type": "code",
      "source": [
        "########## PART 4 - TESTING THE SEQ2SEQ MODEL ##########\n",
        " \n",
        " \n",
        "with tf.Session(graph=train_graph) as session:\n",
        "    # Loading the weights and Running the session\n",
        "    checkpoint = \"checkpoints/chatbot_weights.ckpt\"\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    tf.train.Saver().restore(session, checkpoint)\n",
        "\n",
        "    # Converting the questions from strings to lists of encoding integers\n",
        "    def convert_string2int(question, word2int):\n",
        "        question = clean_text(question)\n",
        "        return [word2int.get(word, word2int['<OUT>']) for word in question.split()]\n",
        "\n",
        "    # Setting up the chat\n",
        "    while(True):\n",
        "        question = input(\"You: \")\n",
        "        if question == 'Goodbye':\n",
        "            break\n",
        "        input_data = train_graph.get_tensor_by_name('input:0')\n",
        "        logits = train_graph.get_tensor_by_name('predictions:0')\n",
        "        target_sequence_length = train_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "        source_sequence_length = train_graph.get_tensor_by_name('source_sequence_length:0')\n",
        "        keep_prob = train_graph.get_tensor_by_name('keep_prob:0')\n",
        "        question = convert_string2int(question, questionswords2int)\n",
        "        question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
        "        fake_batch = np.zeros((batch_size, 25))\n",
        "        fake_batch[0] = question\n",
        "        predicted_answer = session.run(inference_logits, {input_data: fake_batch, \n",
        "                                                          target_sequence_length: [len(question)]*batch_size,\n",
        "                                                          source_sequence_length: [len(question)]*batch_size,\n",
        "                                                          keep_prob: 0.5})[0]\n",
        "        answer = ''\n",
        "        for i in predicted_answer[0]:\n",
        "            if answersints2word[i] == 'i':\n",
        "                token = ' I'\n",
        "            elif answersints2word[i] == '<EOS>':\n",
        "                token = '.'\n",
        "            elif answersints2word[i] == '<OUT>':\n",
        "                token = 'out'\n",
        "            else:\n",
        "                token = ' ' + answersints2word[i]\n",
        "            answer += token\n",
        "            if token == '.':\n",
        "                break\n",
        "        print('ChatBot: ' + answer)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints/chatbot_weights.ckpt\n",
            "You: Hi\n",
            "ChatBot: outout I what I\n",
            "You: how are you\n",
            "ChatBot:  they the they the the\n",
            "You: good morning\n",
            "ChatBot:  they I I I I\n",
            "You: have a great day\n",
            "ChatBot:  theoutout youout\n",
            "You: let me know if you need anything\n",
            "ChatBot:  what what that I I\n",
            "You: its great to know that the model is working.\n",
            "ChatBot:  the what what what what\n",
            "You: hopefully we cam increase the accuracy and make you speak better\n",
            "ChatBot:  what what what what what\n",
            "You: Goodbye\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}